{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8b5431-ecb5-44a4-bc3a-de545d5b55cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b86fb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('../../../datasets/summary_general-2.txt', sep=r'\\s*\\|\\s*', engine='python')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Remove rows with NaN values\n",
    "data = data.dropna()\n",
    "\n",
    "# Create additional features\n",
    "data['Normalized_T50'] = data['T50'] / data['T90']\n",
    "data['Log_T50'] = np.log(data['T50'])\n",
    "data['T50_to_T90'] = data['T50'] / data['T90']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f007e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the feature matrix for clustering\n",
    "features = ['T50', 'Normalized_T50', 'Log_T50', 'T50_to_T90']\n",
    "X = data[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a4b5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the feature columns\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5389d5e4",
   "metadata": {},
   "source": [
    "Option 1: K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b21208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the K-means clustering\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "# Predict the clusters\n",
    "clusters = kmeans.predict(X_scaled)\n",
    "\n",
    "# Add the cluster labels to the dataset\n",
    "data['Cluster'] = clusters\n",
    "\n",
    "# Evaluate the clustering with silhouette score\n",
    "silhouette_avg = silhouette_score(X_scaled, clusters)\n",
    "print(f'Silhouette Score: {silhouette_avg:.2f}')\n",
    "\n",
    "# Display the updated dataset with cluster labels\n",
    "#print(data.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2876070",
   "metadata": {},
   "source": [
    "Option 2: Isolation Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cb6a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c493bb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit the Isolation Forest model\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42)  # Adjust contamination as needed\n",
    "clusters_iso = iso_forest.fit_predict(X_scaled)\n",
    "\n",
    "# Convert -1 (anomalies) to 1 and 1 (inliers) to 0 for easier interpretation\n",
    "data['Cluster_IsolationForest'] = np.where(clusters_iso == -1, 1, 0)\n",
    "\n",
    "# Evaluate the results (if you have ground truth labels)\n",
    "# For this example, we'll assume you don't have ground truth, so this step is optional\n",
    "# If you had labels, you could evaluate as follows:\n",
    "# y_true = data['Ground_Truth_Label']  # Replace with actual ground truth labels\n",
    "# y_pred = data['Cluster_IsolationForest']\n",
    "# accuracy = accuracy_score(y_true, y_pred)\n",
    "# print(f'Accuracy: {accuracy:.2f}')\n",
    "# print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Display the updated dataset with cluster labels\n",
    "#print(data.head())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3209136",
   "metadata": {},
   "source": [
    "Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5870cf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have reduced to 2D or have 2 features for visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot normal data points\n",
    "plt.scatter(X_scaled[data['Cluster_IsolationForest'] == 0, 0], \n",
    "            X_scaled[data['Cluster_IsolationForest'] == 0, 1], \n",
    "            c='blue', label='Normal')\n",
    "\n",
    "# Plot anomalies\n",
    "plt.scatter(X_scaled[data['Cluster_IsolationForest'] == 1, 0], \n",
    "            X_scaled[data['Cluster_IsolationForest'] == 1, 1], \n",
    "            c='red', label='Anomaly')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Isolation Forest Anomaly Detection')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fb23fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2299fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensions to 2D for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot normal data points\n",
    "plt.scatter(X_pca[data['Cluster_IsolationForest'] == 0, 0], \n",
    "            X_pca[data['Cluster_IsolationForest'] == 0, 1], \n",
    "            c='blue', label='Normal')\n",
    "\n",
    "# Plot anomalies\n",
    "plt.scatter(X_pca[data['Cluster_IsolationForest'] == 1, 0], \n",
    "            X_pca[data['Cluster_IsolationForest'] == 1, 1], \n",
    "            c='red', label='Anomaly')\n",
    "\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('2D Projection of Isolation Forest Results')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4724a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add anomaly labels to the dataset\n",
    "data['Anomaly'] = np.where(data['Cluster_IsolationForest'] == 1, 'Anomaly', 'Normal')\n",
    "\n",
    "# Create a pairplot\n",
    "sns.pairplot(data, hue='Anomaly', vars=['T50', 'Normalized_T50', 'Log_T50', 'T50_to_T90'], palette={'Anomaly': 'red', 'Normal': 'blue'})\n",
    "plt.title('Pairplot with Anomalies Highlighted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71035248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have three features, e.g., T50, Normalized_T50, and Log_T50\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot normal data points\n",
    "ax.scatter(X_scaled[data['Cluster_IsolationForest'] == 0, 0], \n",
    "           X_scaled[data['Cluster_IsolationForest'] == 0, 1], \n",
    "           X_scaled[data['Cluster_IsolationForest'] == 0, 2], \n",
    "           c='blue', label='Normal', alpha=0.6)\n",
    "\n",
    "# Plot anomalies\n",
    "ax.scatter(X_scaled[data['Cluster_IsolationForest'] == 1, 0], \n",
    "           X_scaled[data['Cluster_IsolationForest'] == 1, 1], \n",
    "           X_scaled[data['Cluster_IsolationForest'] == 1, 2], \n",
    "           c='red', label='Anomaly', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_zlabel('Feature 3')\n",
    "plt.title('3D Plot of Isolation Forest Results')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b3c580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute anomaly scores\n",
    "anomaly_scores = iso_forest.decision_function(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(anomaly_scores[data['Cluster_IsolationForest'] == 0], bins=30, alpha=0.6, color='blue', label='Normal')\n",
    "plt.hist(anomaly_scores[data['Cluster_IsolationForest'] == 1], bins=30, alpha=0.6, color='red', label='Anomaly')\n",
    "plt.xlabel('Anomaly Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Anomaly Scores')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
